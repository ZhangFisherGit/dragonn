{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kundajelab/dragonn/blob/master/paper_supplement/PrimerTutorial%205%20-%20Functional%20variant%20characterization%20for%20non-coding%20SNPs%20within%20the%20SPI1%20motif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations: \n",
    "# Confusion matrix representation (observed: -1, 0, +1 vs predicted -1, 0 ,+1)\n",
    "# * include all SNPs \n",
    "# * bQTL's will fall in +/-1 bins \n",
    "# Enrichment curves: model vs motifs \n",
    "# Scatterplot: model vs motifs \n",
    "\n",
    "#Weak affinity vs strong affinity (examples : SNP disrupts motif center, SNP disrupts flanking region) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIu-8nkCV4zm"
   },
   "source": [
    "# How to train your DragoNN tutorial 5: \n",
    "## Functional variant characterization for non-coding SNPs within the SPI1 motif \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data: SPI1 ChiP-seq and experimental bQTL data</a></li>\n",
    "    <li><a href=#2>Genomewide classification and regression labels for SPI1 TF ChiPseq</a></li>\n",
    "    <li><a href=#3>Optional: Download pre-generated models and test-set predictions</a></li>\n",
    "    <li><a href=#4>Genome-wide classification for SPI1</a></li>\n",
    "    <li><a href=#5>Calibrate model predictions with Platt Scaling</a></li>\n",
    "    <li><a href=#6>Genome-wide regression for SPI1</a></li> \n",
    "    <li><a href=#7>Calibrate model predictions with Isotonic Regression</a></li> \n",
    "    <li><a href=#8>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT</a></li>\n",
    "    <li><a href=#9>Recovering bQTL effect sizes: Classification vs Regression</a></li>\n",
    "    <li><a href=#10>Model-predicted SNP effect sizes vs bQTL effect sizes</a></li>\n",
    "    <li><a href=#11>Conclusions</a></li>    \n",
    "    <li><a href=#12>Save tutorial outputs</a></li>\n",
    "</ol>\n",
    "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72XlYRZBluGr"
   },
   "outputs": [],
   "source": [
    "# If you don't have bedtools installed in your environment (i.e. Google Colab), uncomment and run the command below \n",
    "#!apt-get install bedtools\n",
    "#!pip install pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmftiCCDV4zo"
   },
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install dragonn>=0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyLzeiF5V4zq"
   },
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8M6gdfuJV4zu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djDLAi21V4zy"
   },
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial uses the same in vivo SPI1 transcription factor CHiP-seq dataset that was used in [Tutorial 4](https://colab.research.google.com/github/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/PrimerTutorial%204%20-%20Interpreting%20predictive%20sequence%20features%20in%20in-vivo%20TF%20binding%20events.ipynb). Our goal is to compare predicted variant effect sizes from classification and regression models against experimental bQTL data. The bQTL data in this way serves as a \"gold-standard\" validation that in silico mutagenesis on the deep learning inputs leads to correct variant effect size prediction.  We  will use bQTL data  that has been intersected with SPI1 CISBP genome motif annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "O707uf21V4zy",
    "outputId": "f5ca2dbc-9594-4a62-aa67-97190945d622"
   },
   "outputs": [],
   "source": [
    "# SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "!wget -O SPI1.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "#Fold change bigWig track for the SPI1 dataset: \n",
    "!wget -O SPI1.pooled.fc.bigWig http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-macs2_pooled/execution/ENCFF000OBU.Rep1.merged.nodup.pooled_x_ENCFF000OCW.Control.Rep1.merged.nodup.fc.signal.bigwig\n",
    "    \n",
    "## Download \"ambiguous\" peak sets -- these peaks are in the optimal overlap set across replicates, but are not\n",
    "## found to be reproducible at a high confidence (p<0.05) by IDR \n",
    "! wget -O SPI1.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.ambiguous.gz\n",
    "\n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.fai \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.gzi \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "-YwnqCV-V4z2",
    "outputId": "85791b16-8647-45ff-cd8b-cf281d618350"
   },
   "outputs": [],
   "source": [
    "# Download bQTL experimental data for SPI1 loci \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sp9mi-6_V4z4"
   },
   "source": [
    "## Generating genome-wide classification and regression labels <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zmt5OJP_V4z5"
   },
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLGpH2rOV4z6"
   },
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "u2wpRugxV4z7",
    "outputId": "c695d444-b6d0-408b-f294-6afcdc6b1033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPI1\tSPI1.narrowPeak.gz\tSPI1.pooled.fc.bigWig\tSPI1.ambiguous.gz\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call SPI1.tasks.tsv, with task names in column 1, corresponding\n",
    "## peak files in column 2, and the signal track in column 3. In this tutorial, the task file will have a single task entry for the SPI1 TF CHiP-seq\n",
    "with open(\"SPI1.task.tsv\",'w') as f: \n",
    "    f.write(\"SPI1\\tSPI1.narrowPeak.gz\\tSPI1.pooled.fc.bigWig\\tSPI1.ambiguous.gz\\n\")\n",
    "f.close() \n",
    "!cat SPI1.task.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pqz2oVGV4z_"
   },
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* In the classification case, each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. The bin is labeled negative if there is no overlap with the narrowPeak. \n",
    "* In the regression case, the asinh(mean coverage) in the 200 bp bin is computed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e--f8QWuV4z_"
   },
   "source": [
    "**Note**: The label generation may take 10 - 15 minutes to complete. If you prefer not to wait, you can download the \n",
    "pre-generated classification and regression labels for the training, validation, and test sets by uncommenting the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1205
    },
    "colab_type": "code",
    "id": "LMG9IzPnV40A",
    "outputId": "aca1e05d-dc62-416e-b22f-b68a53aaf3f7"
   },
   "outputs": [],
   "source": [
    "## Classification labels \n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.classification.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.classification.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.classification.hdf5\n",
    "\n",
    "## Regression labels \n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
    "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lleRdAaV40B"
   },
   "source": [
    "If you prefer to generate the labels from scratch, execute the two code cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erh3B4hIV40D"
   },
   "outputs": [],
   "source": [
    "#  Generate genome-wide classification labels \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set. Also, the dataset does not\n",
    "# include chromosome Y, so we exclude it as well. \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.classification.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBqbysuAV40G"
   },
   "outputs": [],
   "source": [
    "# Generate regression labels genome-wide \n",
    "\n",
    "#1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "train_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.train.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':4,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Validation set: Chromosome 1\n",
    "valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.valid.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':1,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "#3) Test set: Chromosomes 2, 19 \n",
    "test_set_params={\n",
    "    'task_list':\"SPI1.task.tsv\",\n",
    "    'outf':\"SPI1.test.regression.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':2,\n",
    "    'subthreads':4,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'all_genome_bins_regression'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1m9HsgXV40J"
   },
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "Q0SgKnZtV40J",
    "outputId": "5e5501e6-61e3-44e1-f6ca-94e70f3ff63f"
   },
   "outputs": [],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"SPI1.train.classification.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "weH35tmhV40N",
    "outputId": "a8aea293-f560-400f-cb2f-2d0085625934"
   },
   "outputs": [],
   "source": [
    "pd.read_hdf(\"SPI1.train.regression.hdf5\",start=1000000,stop=1000010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKHBBFpRV40Q"
   },
   "source": [
    "## Optional: Download pre-generated models and test-set predictions <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Next, we will train classification and regression models to predict TF CHiP-seq peaks for SPI1. If you want to skip straight to model interpretation and bQTL analysis, you can download the pre-trained models by uncommenting the \n",
    "block of code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqyIROINV40R"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "from dragonn.custom_losses import get_ambig_binary_crossentropy, get_ambig_mean_squared_error\n",
    "\n",
    "## Download classification model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.classification.model.hdf5\n",
    "spi1_classification_model=load_model(\"SPI1.classification.model.hdf5\", custom_objects={\"ambig_binary_crossentropy\":get_ambig_binary_crossentropy()})\n",
    "\n",
    "## Download regression model \n",
    "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.regression.model.hdf5\n",
    "spi1_regression_model=load_model(\"SPI1.regression.model.hdf5\",custom_objects={\"ambig_mean_squared_error\":get_ambig_mean_squared_error()})\n",
    "\n",
    "\n",
    "## Get test set classification model and regression model predictions \n",
    "#import h5py\n",
    "#test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\")\n",
    "#spi1_test_classification_predictions=test_set_predictions['classification'].value \n",
    "#spi1_test_regression_predictions=test_set_predictions['regression'].value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZcwz5AmV40U"
   },
   "source": [
    "## Genome-wide classification model <a name='4'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeBBukYGV40V"
   },
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXtJhYf2V40Z"
   },
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "from keras.constraints import max_norm\n",
    "from dragonn.custom_losses import get_ambig_binary_crossentropy\n",
    "def initialize_classification_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    #use the custom ambig_binary_crossentropy loss, indicating that a value of np.nan indicates an ambiguous label \n",
    "    loss=get_ambig_binary_crossentropy()\n",
    "    \n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam', loss=loss,\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XY-g6ik9V40c"
   },
   "source": [
    "We create generators for the training and validation data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5PrSMLLiV40d",
    "outputId": "d7ae2438-6e8e-4fc0-c1b3-6bde39667604"
   },
   "outputs": [],
   "source": [
    "#create the generators, upsample positives to ensure they constitute 30% of each batch \n",
    "from dragonn.generators import * \n",
    "spi1_train_classification_gen=DataGenerator(\"SPI1.train.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3, batch_size=256)\n",
    "spi1_valid_classification_gen=DataGenerator(\"SPI1.valid.classification.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.01, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLRfBhebV40e"
   },
   "outputs": [],
   "source": [
    "#Train the SPI1 classification model \n",
    "spi1_classification_model=initialize_classification_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_classification=spi1_classification_model.fit_generator(spi1_train_classification_gen,\n",
    "                                                  validation_data=spi1_valid_classification_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DN2bFz_aV40h",
    "outputId": "323bc266-fc71-4e30-e277-b2db3c3e9a0a"
   },
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoP336y_V40k"
   },
   "source": [
    "We now measure how well the model performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZF6VNZH3V40k",
    "outputId": "101ca0a8-db49-44cc-c133-e799b070e1e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6047/6047 [==============================] - 384s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "from dragonn.generators import * \n",
    "spi1_test_classification_gen=DataGenerator(\"SPI1.test.classification.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_classification_predictions=spi1_classification_model.predict_generator(spi1_test_classification_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_classification_truth=spi1_test_classification_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6046528, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spi1_test_classification_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nans, as they corresponnd to ambiguous values \n",
    "nan_indices=np.isnan(spi1_test_classification_truth['SPI1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_truth=spi1_test_classification_truth[~nan_indices]\n",
    "spi1_test_classification_predictions=spi1_test_classification_predictions[~nan_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X83Mt6VXV40t",
    "outputId": "b7b4fe79-812d-463a-f2f1-611122a6fa0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0886\tBalanced Accuracy: 92.79%\t auROC: 0.986\t auPRC Careful: 0.375\t auPRC Trapezoidal: 0.375\n",
      "\tRecall at 5%|10%|20% FDR: 0.0%|0.0%|4.1%\t Num Positives: 20574\t Num Negatives: 6015749\n"
     ]
    }
   ],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_classification_truth.values.astype(bool),spi1_test_classification_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate the model predictions with Platt scaling  <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit,expit \n",
    "spi1_test_classification_logits=logit(spi1_test_classification_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abstention \n",
    "from abstention.calibration import PlattScaling, IsotonicRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platt scaling coef: 1.034707666031409 ; intercept: -4.429173167072346\n"
     ]
    }
   ],
   "source": [
    "classification_calibration_func = PlattScaling()(\n",
    "                            valid_preacts=spi1_test_classification_logits,\n",
    "                            valid_labels=spi1_test_classification_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_classification_predictions_calibrated=classification_calibration_func(spi1_test_classification_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0107\tBalanced Accuracy: 60.55%\t auROC: 0.986\t auPRC Careful: 0.375\t auPRC Trapezoidal: 0.375\n",
      "\tRecall at 5%|10%|20% FDR: 0.0%|0.0%|4.3%\t Num Positives: 20574\t Num Negatives: 6015749\n"
     ]
    }
   ],
   "source": [
    "## Generate a ClassificationResult object on calibrated model predictions \n",
    "print(ClassificationResult(spi1_test_classification_truth.values.astype(bool),\n",
    "                           np.expand_dims(spi1_test_classification_predictions_calibrated,axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0sibNElV403"
   },
   "source": [
    "## Genome-wide regression model <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM0-1aa9V404"
   },
   "outputs": [],
   "source": [
    "from dragonn.custom_losses import get_ambig_mean_squared_error\n",
    "def initialize_regression_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "\n",
    "    loss=get_ambig_mean_squared_error()\n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam',loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zO-uKK8GV407",
    "outputId": "04c1a178-b8ee-4b14-c944-328c17ecec9c"
   },
   "outputs": [],
   "source": [
    "#we want to determine a threshold for upsampling the non-zero bins in a given batch \n",
    "# extract 5 million datapoints from the training data and observe the distribution of non-zero signal values  \n",
    "sample=pd.read_hdf(\"SPI1.train.regression.hdf5\",start=0,stop=5000000)\n",
    "nonzero_sample=sample[sample.max(axis=1)>0]\n",
    "print(nonzero_sample.shape)\n",
    "nonzero_sample.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5Raqyz3V40_"
   },
   "source": [
    "This suggests that 1 is a reasonable threshold for upsampling signal bins in regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSjGIlYTV41B"
   },
   "outputs": [],
   "source": [
    "#create the generators, no upsampling of positives is used for regression. \n",
    "from dragonn.generators import * \n",
    "spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=1)\n",
    "spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.01,upsample_thresh=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3pSs4TYV41E",
    "outputId": "bb50b654-2336-4b6d-d16c-a8a15672f379"
   },
   "outputs": [],
   "source": [
    "#Train the SPI1 regression model \n",
    "spi1_regression_model=initialize_regression_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
    "                                                  validation_data=spi1_valid_regression_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFxneNJqV41H",
    "outputId": "472b64dc-f85d-49e6-8bd0-4d023374e04a"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o3I4gN7V41K",
    "outputId": "da95db30-a6ad-4043-86c6-ed587a44277b"
   },
   "outputs": [],
   "source": [
    "from dragonn.generators import * \n",
    "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.hdf5\",\n",
    "                                       \"hg19.genome.fa.gz\",\n",
    "                                         upsample=False,\n",
    "                                         add_revcomp=False,\n",
    "                                         batch_size=1000,\n",
    "                                         tasks=['SPI1'])\n",
    "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "spi1_test_regression_truth=spi1_test_regression_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nans, as they corresponnd to ambiguous values \n",
    "spi1_test_regression_truth=spi1_test_regression_truth[~nan_indices]\n",
    "spi1_test_regression_predictions=spi1_test_regression_predictions[~nan_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model calibration with Isotonic Regression  <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_calibration_func = IsotonicRegression()(\n",
    "    valid_preacts=spi1_test_regression_predictions,\n",
    "    valid_labels=spi1_test_regression_truth['SPI1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_predictions_calibrated=regression_calibration_func(spi1_test_regression_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi1_test_regression_predictions_calibrated=np.expand_dims(spi1_test_regression_predictions_calibrated,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMbEZo8XGd2m"
   },
   "outputs": [],
   "source": [
    "## find the indices of the non-zero coverage bins \n",
    "nonzero_bins=spi1_test_regression_truth.max(axis=1)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-byNK4qMV41N",
    "outputId": "aa18b7e6-6ee2-439d-ca0b-d68d2a7aa8a1"
   },
   "outputs": [],
   "source": [
    "#Calculate spearman and pearson correlation between truth labels and predictions \n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "corr_pearson=pearsonr(spi1_test_regression_truth,spi1_test_regression_predictions_calibrated)\n",
    "corr_spearman=spearmanr(spi1_test_regression_truth,spi1_test_regression_predictions_calibrated)\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YipLiXRFGd2t",
    "outputId": "f7e2d334-30b4-4661-d8bb-23832c8c5763"
   },
   "outputs": [],
   "source": [
    "# Calculate the spearman and pearson correlation, restricted to non-zero bins \n",
    "corr_pearson_nonzero_bins=pearsonr(spi1_test_regression_truth[nonzero_bins],spi1_test_regression_predictions_calibrated[nonzero_bins])\n",
    "corr_spearman_nonzero_bins=spearmanr(spi1_test_regression_truth[nonzero_bins],spi1_test_regression_predictions_calibrated[nonzero_bins])\n",
    "print(\"Pearson correlation on test set:\"+str(corr_pearson_nonzero_bins))\n",
    "print(\"Spearman correlation on test set:\"+str(corr_spearman_nonzero_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGW2FQrgGd2y",
    "outputId": "e3069d0a-65d3-4c8c-be0d-eef77ed83581"
   },
   "outputs": [],
   "source": [
    "spi1_test_regression_truth.values[0:10].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKa34qjbGd21"
   },
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame({\"Observed\":list(spi1_test_regression_truth.values.squeeze()),\n",
    "                     \"Predicted\":list(spi1_test_regression_predictions_calibrated.squeeze())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1tUvhU5Gd27"
   },
   "outputs": [],
   "source": [
    "test_df_nonzero=pd.DataFrame({\"Observed\":list(spi1_test_regression_truth[nonzero_bins].values.squeeze()),\n",
    "                     \"Predicted\":list(spi1_test_regression_predictions_calibrated[nonzero_bins].squeeze())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xqvik10lGd29",
    "outputId": "4d35b751-13ce-4555-f798-916765d4e981"
   },
   "outputs": [],
   "source": [
    "import plotnine \n",
    "from plotnine import * \n",
    "print((ggplot(test_df,aes(x=\"Observed\",y=\"Predicted\"))\n",
    " +geom_bin2d(bins=100)\n",
    " +theme_bw()\n",
    " +xlab(\"Observed asinh(mean coverage in FC bigWig\")\n",
    " +ylab(\"Model prediction\")\n",
    " +ggtitle(\"SPI1 regression model test set prediction\")))\n",
    "\n",
    "print((ggplot(test_df_nonzero,aes(x=\"Observed\",y=\"Predicted\"))\n",
    " +geom_bin2d(bins=100)\n",
    " +theme_bw()\n",
    " +xlab(\"Observed asinh(mean coverage in FC bigWig\")\n",
    " +ylab(\"Model prediction\")\n",
    " +ggtitle(\"SPI1 regression model test set prediction: bins with nonzero coverage\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48jGNSBtV41R"
   },
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='8'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWumi0mQV41S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                          SPI1\n",
       "CHR   START    END           \n",
       "chr2  191500   192500     1.0\n",
       "      191550   192550     1.0\n",
       "      191600   192600     1.0\n",
       "      464500   465500     1.0\n",
       "      464550   465550     1.0\n",
       "      464600   465600     1.0\n",
       "      464650   465650     1.0\n",
       "      1135200  1136200    1.0\n",
       "      1135250  1136250    1.0\n",
       "      1135300  1136300    1.0\n",
       "      3483850  3484850    1.0\n",
       "      3483900  3484900    1.0\n",
       "      6485250  6486250    1.0\n",
       "      6485300  6486300    1.0\n",
       "      6485350  6486350    1.0\n",
       "      6485400  6486400    1.0\n",
       "      6492050  6493050    1.0\n",
       "      7551500  7552500    1.0\n",
       "      7765350  7766350    1.0\n",
       "      7975500  7976500    1.0\n",
       "      8163050  8164050    1.0\n",
       "      8163100  8164100    1.0\n",
       "      8163150  8164150    1.0\n",
       "      8198550  8199550    1.0\n",
       "      10107100 10108100   1.0\n",
       "      10107150 10108150   1.0\n",
       "      10107200 10108200   1.0\n",
       "      10107250 10108250   1.0\n",
       "      10885300 10886300   1.0\n",
       "      10885350 10886350   1.0\n",
       "...                       ...\n",
       "chr19 52124200 52125200   1.0\n",
       "      52314200 52315200   1.0\n",
       "      52314250 52315250   1.0\n",
       "      52314350 52315350   1.0\n",
       "      52419050 52420050   1.0\n",
       "      52419100 52420100   1.0\n",
       "      52419150 52420150   1.0\n",
       "      52419200 52420200   1.0\n",
       "      53749350 53750350   1.0\n",
       "      53749500 53750500   1.0\n",
       "      54850050 54851050   1.0\n",
       "      54850650 54851650   1.0\n",
       "      54850700 54851700   1.0\n",
       "      55427900 55428900   1.0\n",
       "      55428000 55429000   1.0\n",
       "      55469900 55470900   1.0\n",
       "      55469950 55470950   1.0\n",
       "      55470000 55471000   1.0\n",
       "      55470050 55471050   1.0\n",
       "      55517000 55518000   1.0\n",
       "      55517050 55518050   1.0\n",
       "      55765150 55766150   1.0\n",
       "      55896750 55897750   1.0\n",
       "      55896800 55897800   1.0\n",
       "      56170900 56171900   1.0\n",
       "      56170950 56171950   1.0\n",
       "      56189150 56190150   1.0\n",
       "      56189200 56190200   1.0\n",
       "      56189250 56190250   1.0\n",
       "      56189300 56190300   1.0\n",
       "\n",
       "[966 rows x 1 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
    "spi1_test_classification_truth_bool=spi1_test_classification_truth.values.astype(bool)\n",
    "spi1_test_classification_predictions_calibrated=np.expand_dims(spi1_test_classification_predictions_calibrated,axis=1)\n",
    "true_pos_spi1=spi1_test_classification_truth[spi1_test_classification_truth_bool*spi1_test_classification_predictions_calibrated >0.9]\n",
    "true_pos_spi1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UH5WvDmXV41W",
    "outputId": "c549e164-1087-4f2b-93bd-7f0677075090"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(966, 1, 1000, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41dInIg-V41Y"
   },
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"deep_lift_input_classification_spi1\",deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hViDtFpGV41a"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a613bc4399b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeep_lift_scores_spi1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeeplift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SPI1.classification.model.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeep_lift_input_spi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dragonn/dragonn/tutorial_utils.py\u001b[0m in \u001b[0;36mdeeplift\u001b[0;34m(model, X, batch_size, target_layer_idx, task_idx, num_refs_per_seq, reference)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"shuffled_ref\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdeeplift_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeeplift_shuffled_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_refs_per_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"gc_ref\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mdeeplift_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeeplift_gc_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dragonn/dragonn/tutorial_utils.py\u001b[0m in \u001b[0;36mdeeplift_shuffled_ref\u001b[0;34m(X, score_func, batch_size, task_idx, num_refs_per_seq)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mnum_refs_per_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_refs_per_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         progress_update=None)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeeplift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_layer_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_refs_per_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shuffled_ref\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/deeplift/util.py\u001b[0m in \u001b[0;36mcompute_scores_with_shuffle_seq_refs\u001b[0;34m(task_idx, input_data_sequences, num_refs_per_seq, batch_size, seed, progress_update)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             [len(input_data_sequences),\n\u001b[1;32m    390\u001b[0m                              num_refs_per_seq]\n\u001b[0;32m--> 391\u001b[0;31m                              +list(input_data_list[0].shape[1:])) \n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;31m#take the mean over all the refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mmean_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "deep_lift_scores_spi1=deeplift(\"SPI1.classification.model.hdf5\",deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFkS3UwpV41c",
    "outputId": "953cdbc2-d7c0-4a17-d6d1-227acb47c325"
   },
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skCZQ8j0V41e"
   },
   "source": [
    "Let's plot a few of the DeepLIFT tracks and see if the model successfully learned SPI1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Srt-dPa_V41e"
   },
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import  plot_seq_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhcSoq48V41h",
    "outputId": "fd5de43b-d65f-4abb-d998-7115c5ad1c1e"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyejvCeWV41j",
    "outputId": "9985d83c-d05f-4979-f472-bb352cce2f80"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyuioSW_V41o",
    "outputId": "75f83e18-bf46-475d-c43b-22ab8cdc56d9"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfkuGJTGV41r"
   },
   "source": [
    "Let's zoom in to the center of one sequence so that it is easier to distinguish the motif: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCr3vtPeV41s",
    "outputId": "a04a10b2-ff18-44cb-f918-005872cb3b59"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[550:650],deep_lift_input_spi1[2].squeeze()[550:650])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tiw5YPInV41v"
   },
   "source": [
    "If we query the sequence \"CACTTCCCCT\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match for SPIB: \n",
    "<img src=\"https://github.com/kundajelab/dragonn/blob/master/paper_supplement/tutorial_images/SPI1.Tut4.png?raw=1\" alt=\"SPI12TomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUxg-r1V41x"
   },
   "source": [
    "### Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEJudwWXV41y",
    "outputId": "f2395f38-be1f-4b76-95a3-91cc85efa471"
   },
   "outputs": [],
   "source": [
    "#Sanity-check that the model is learning the SPI1 motif by running DeepLIFT on True Positives with high confidence (>0.9)\n",
    "#get the true positive predictions \n",
    "true_pos=spi1_test_regression_truth[(spi1_test_regression_truth.values*spi1_test_regression_predictions_calibrated)>2]\n",
    "true_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5dpuvzLV413",
    "outputId": "c0db98aa-22bf-4e44-f15f-0a75df7c9239"
   },
   "outputs": [],
   "source": [
    "deep_lift_input=one_hot_from_bed([i for i in true_pos.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-3kc90ZV416",
    "outputId": "514a9b2b-0f73-45ac-fe0c-b695bb182dbb"
   },
   "outputs": [],
   "source": [
    "help(deeplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeuZFMZqV41-"
   },
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(spi1_regression_model,deep_lift_input_spi1,target_layer_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qigmzDOV41-",
    "outputId": "1ef6a324-0b04-4000-a658-87c75cb0c50d"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPHe9I8VV42A",
    "outputId": "b7ee3839-2d4e-466b-e2f7-ede00db8a16b"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bogEKZN2V42C",
    "outputId": "6465c01d-b842-4217-9766-aca7a14797b2"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ck63kAsAV42F",
    "outputId": "5353b1fb-b881-4515-f931-96f49f196045"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[550:650],deep_lift_input_spi1[2].squeeze()[550:650])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWqxtR6NV42I"
   },
   "source": [
    "The motif learned by the regression model matches the canonical SPI1 motif, though the deepLIFT tracks are noisier compared to those for the classification model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PGA_k3RV42J"
   },
   "source": [
    "## Recovering bQTL effect sizes: Classification vs Regression <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GROAoPZDV42J"
   },
   "outputs": [],
   "source": [
    "from dragonn.generators import * \n",
    "bqtl_ref_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"POSTallele\")\n",
    "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"ALTallele\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqtl_ref_gen.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJq0Ic_8V42L",
    "outputId": "f27e9d3e-87a2-479f-90ca-320bf3066fc0"
   },
   "outputs": [],
   "source": [
    "bqtl_ref_classification_predictions=spi1_classification_model.predict_generator(bqtl_ref_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlPJpmmLV42L",
    "outputId": "bf8b8ff1-0268-49af-ce7d-9d8875e2ae3f"
   },
   "outputs": [],
   "source": [
    "bqtl_alt_classification_predictions=spi1_classification_model.predict_generator(bqtl_alt_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "bqtl_ref_classification_truth=bqtl_ref_gen.data['pvalue']\n",
    "bqtl_ref_classification_truth=np.expand_dims(bqtl_ref_classification_truth,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGXJaSvPV42N",
    "outputId": "32d58541-15ab-4c82-90d5-b3118ed18520"
   },
   "outputs": [],
   "source": [
    "#sanity check the np.array shapes \n",
    "print(bqtl_ref_classification_predictions.shape)\n",
    "print(bqtl_alt_classification_predictions.shape)\n",
    "print(bqtl_ref_classification_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pre-activations \n",
    "bqtl_ref_classification_logits=logit(bqtl_ref_classification_predictions)\n",
    "bqtl_alt_classification_logits=logit(bqtl_alt_classification_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_max_allowed=np.nanmax(bqtl_ref_classification_logits[np.isfinite(bqtl_ref_classification_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_min_allowed=np.nanmin(bqtl_ref_classification_logits[np.isfinite(bqtl_ref_classification_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_max_allowed=np.nanmax(bqtl_alt_classification_logits[np.isfinite(bqtl_alt_classification_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_min_allowed=np.nanmin(bqtl_alt_classification_logits[np.isfinite(bqtl_alt_classification_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace any infinite values with the maximum/ minimum finite values in the arrays \n",
    "bqtl_ref_classification_logits[bqtl_ref_classification_logits > ref_max_allowed]=ref_max_allowed \n",
    "bqtl_ref_classification_logits[bqtl_ref_classification_logits < ref_min_allowed]=ref_min_allowed \n",
    "bqtl_alt_classification_logits[bqtl_alt_classification_logits > alt_max_allowed]=alt_max_allowed \n",
    "bqtl_alt_classification_logits[bqtl_alt_classification_logits < alt_min_allowed]=alt_min_allowed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibrate predictions with Platt Scaling \n",
    "bqtl_ref_classification_predictions_calibrated=classification_calibration_func(bqtl_ref_classification_logits)\n",
    "bqtl_alt_classification_predictions_calibrated=classification_calibration_func(bqtl_alt_classification_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NpOM03tV42P",
    "outputId": "9e58fdff-c60a-463a-fa43-2a6979d96064"
   },
   "outputs": [],
   "source": [
    "bqtl_ref_regression_predictions=spi1_regression_model.predict_generator(bqtl_ref_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n",
    "bqtl_alt_regression_predictions=spi1_regression_model.predict_generator(bqtl_alt_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate predictions with isotonic regression \n",
    "bqtl_ref_regression_predictions_calibrated=regression_calibration_func(bqtl_ref_regression_predictions)\n",
    "bqtl_alt_regression_predictions_calibrated=regression_calibration_func(bqtl_alt_regression_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hi_Pr6jcV42Q",
    "outputId": "62710f23-734d-4ea4-b9a9-e1c28948d597"
   },
   "outputs": [],
   "source": [
    "plt.scatter(bqtl_ref_classification_predictions_calibrated, bqtl_alt_classification_predictions_calibrated, s=0.1, alpha=0.01)\n",
    "plt.xlabel(\"Ref\")\n",
    "plt.ylabel(\"Alt\")\n",
    "plt.title(\"BQTL Classification Model Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tg-ZRo1tV42R",
    "outputId": "9cb78eaf-ed5f-4fe4-ca02-6f927b07bcca"
   },
   "outputs": [],
   "source": [
    "plt.scatter(bqtl_ref_regression_predictions_calibrated, bqtl_alt_regression_predictions_calibrated, s=0.1, alpha=0.01)\n",
    "plt.xlabel(\"Ref\")\n",
    "plt.ylabel(\"Alt\")\n",
    "plt.title(\"BQTL Regression Model Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(-10*np.log10(bqtl_ref_classification_truth), bqtl_ref_regression_predictions_calibrated, s=0.1, alpha=0.01)\n",
    "plt.xlabel(\"-10log10(P-value)\")\n",
    "plt.ylabel(\"Ref Regression Calibrated\")\n",
    "plt.title(\"BQTL Regression Model Predictions\")\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save bqtl predictions \n",
    "chroms=[i[0] for i in bqtl_ref_gen.data.index]\n",
    "pos=[i[1] for i in bqtl_ref_gen.data.index]\n",
    "import h5py \n",
    "bqtl_predictions=h5py.File(\"SPI1.BQTL.predictions.hdf5\",'w')\n",
    "bqtl_predictions.create_dataset(\"bqtl_chrom\",data=np.string_(chroms))\n",
    "bqtl_predictions.create_dataset(\"bqtl_position\",data=np.asarray(pos))\n",
    "bqtl_predictions.create_dataset(\"bqtl_pvalue\",data=bqtl_ref_gen.data['pvalue'])\n",
    "bqtl_predictions.create_dataset(\"bqtl_classification_ref_preactivation\",data=bqtl_ref_classification_logits)\n",
    "bqtl_predictions.create_dataset(\"bqtl_classification_ref_postactivation\",data=bqtl_ref_classification_predictions)\n",
    "bqtl_predictions.create_dataset(\"bqtl_classification_ref_postactivation_calibrated\",data=bqtl_ref_classification_predictions_calibrated)\n",
    "bqtl_predictions.create_dataset(\"bqtl_classification_alt_preactivation\",data=bqtl_alt_classification_logits)\n",
    "bqtl_predictions.create_dataset(\"bqtl_classification_alt_postactivation\",data=bqtl_alt_classification_predictions)\n",
    "bqtl_predictions.create_dataset(\"bqtl_classification_alt_postactivation_calibrated\",data=bqtl_alt_classification_predictions_calibrated)\n",
    "bqtl_predictions.create_dataset(\"bqtl_regression_ref\",data=bqtl_ref_regression_predictions)\n",
    "bqtl_predictions.create_dataset(\"bqtl_regression_ref_calibrated\",data=bqtl_ref_regression_predictions_calibrated)\n",
    "bqtl_predictions.create_dataset(\"bqtl_regression_alt\",data=bqtl_alt_regression_predictions)\n",
    "bqtl_predictions.create_dataset(\"bqtl_regression_alt_calibrated\",data=bqtl_alt_regression_predictions_calibrated)\n",
    "bqtl_predictions.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMBWHvnZV42V"
   },
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFyKUGQnV42X"
   },
   "source": [
    "## Save tutorial outputs <a name='10'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "We save the models and test set predictions generated in this tutorial to an hdf5 file so that they can be loaded more readily in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBNQNhg3V42X"
   },
   "outputs": [],
   "source": [
    "#save the models \n",
    "spi1_classification_model.save(\"SPI1.classification.model.hdf5\")\n",
    "spi1_regression_model.save(\"SPI1.regression.model.hdf5\")\n",
    "\n",
    "#save the test predictions \n",
    "import h5py \n",
    "test_set_predictions=h5py.File(\"SPI1.test.predictions.hdf5\",'w')\n",
    "test_set_predictions.create_dataset(\"classification_labels\",data=spi1_test_classification_truth)\n",
    "test_set_predictions.create_dataset(\"classification_preactivation\",data=spi1_test_classification_logits)\n",
    "test_set_predictions.create_dataset(\"classification_postactivation\",data=spi1_test_classification_predictions)\n",
    "test_set_predictions.create_dataset(\"classification_postactivation_calibrated\",data=spi1_test_classification_predictions_calibrated)\n",
    "test_set_predictions.create_dataset(\"regression_labels\",data=spi1_test_regression_truth)\n",
    "test_set_predictions.create_dataset(\"regression\",data=spi1_test_regression_predictions)\n",
    "test_set_predictions.create_dataset(\"regression_calibrated\",data=spi1_test_regression_predictions_calibrated)\n",
    "test_set_predictions.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "PrimerTutorial 5 - Functional variant characterization for non-coding SNPs within the SPI1 motif.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
