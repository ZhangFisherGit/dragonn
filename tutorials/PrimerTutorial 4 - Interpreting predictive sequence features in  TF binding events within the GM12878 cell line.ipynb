{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# naive overlap peaks that are not in IDR should be labeled as ambiguous, in addition to flanking bins\n",
    "# barplot of cases 1 -4 , all performance metrics\n",
    "# lower memory footprint for storing/reading hdf5 on Google Colab \n",
    "# Explain that hyperparameter search is performed online in similar way as Tut. 2. \n",
    "# Add calibration for the models. \n",
    "# Move the data zenodo. \n",
    "# Models in kipoi\n",
    "# use nans to indicate ambiguous labels \n",
    "# resample validation to match test set imbalance \n",
    "# add in tensorboard integration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN tutorial 4: \n",
    "## Interpreting predictive sequence features in  TF binding events within the GM12878 cell line. \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript and follows figure 8 in the manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data</a></li>\n",
    "    <li><a href=#2>Generating positive and negative bins for genome-wide training </a></li>\n",
    "    <li><a href=#3>Challenges of in vivo data : batch generators and class upsampling </a></li>\n",
    "    <li><a href=#3.5>Optional: Download pre-generated models and test-set predictions </a></li>\n",
    "    <li><a href=#4>Case 1: Negatives consist of shuffled references, single-tasked models</a></li>  \n",
    "    <li><a href=#5>Case 2: Whole-genome negatives, single-tasked models </a></li>\n",
    "    <li><a href=#6>Case 3: Whole-genome negatives, multi-tasked models </a></li>\n",
    "    <li><a href=#7>Case 4: What happens if we don't upsample positive examples in our batches? </a></li>\n",
    "    <li><a href=#8>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT </a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>    \n",
    "    <li><a href=#10>Save tutorial outputs</a></li>\n",
    "</ol>\n",
    "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have bedtools installed in your environment (i.e. Google Colab), uncomment and run the command below \n",
    "#!apt-get install bedtools\n",
    "#!pip install pybedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install dragonn>=0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Tutorials 1 - 3 have used simulated data generated with the simdna package. In this tutorial, we will examine how well CNN's are able to predict transcription factor binding for four TF's in vivo. \n",
    "\n",
    "We will learn to predict transcription factor binding for four transcription factors in the GM12878 cell line (one of the Tier 1 cell lines for the ENCODE project). First, we download the narrowPeak bed files for each of these transcription factors. You can skip the following code block if you already have the data downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTCF, optimal IDR thresholded peaks, Stam Lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000DRZ/\n",
    "!wget -O CTCF.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/ctcf_ENCSR000DRZ/cromwell-executions/chip/e94d720e-08dd-42fa-bc77-2391e2b8c275/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz \n",
    "\n",
    "## SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "!wget -O SPI1.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "    \n",
    "## ZNF143, optimal IDR thresholded peaks, Snyder lab, hg19\n",
    "#https://www.encodeproject.org/experiments/ENCSR936XTK/\n",
    "!wget -O ZNF143.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/znf143_ENCSR936XTK/cromwell-executions/chip/8e1d0af8-3ca6-44e2-aff0-af83b2c0c061/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "## SIX5, optimal IDR thresholded peaks, Myers Lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BJE/\n",
    "!wget -O SIX5.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/six5_ENCSR00BJE/cromwell-executions/chip/bd36ae6c-a9e3-4d71-a938-c45ab86854fc/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "## Download \"ambiguous\" peak sets -- these peaks are in the optimal overlap set across replicates, but are not\n",
    "## found to be reproducible at a high confidence (p<0.05) by IDR \n",
    "! wget -O CTCF.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/CTCF.ambiguous.gz\n",
    "! wget -O SPI1.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.ambiguous.gz\n",
    "! wget -O ZNF143.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/ZNF143.ambiguous.gz\n",
    "! wget -O SIX5.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SIX5.ambiguous.gz\n",
    "    \n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.fai \n",
    "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.gzi \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating positive and negative bins for genome-wide training <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCF\tCTCF.narrowPeak.gz\t\tCTCF.ambiguous.gz\r\n",
      "SPI1\tSPI1.narrowPeak.gz\t\tSPI1.ambiguous.gz\r\n",
      "SIX5\tSIX5.narrowPeak.gz\t\tSIX5.ambiguous.gz\r\n",
      "ZNF143\tZNF143.narrowPeak.gz\t\tZNF143.ambiguous.gz\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call tasks.tsv, with task names in column 1, the corresponding\n",
    "## peak files in column 2, skip column 3 (which will be used for regression in Tutorial 5), and ambiguous peaks in \n",
    "## column4 \n",
    "with open(\"tasks.tsv\",'w') as f: \n",
    "    f.write(\"CTCF\\tCTCF.narrowPeak.gz\\t\\tCTCF.ambiguous.gz\\n\")\n",
    "    f.write(\"SPI1\\tSPI1.narrowPeak.gz\\t\\tSPI1.ambiguous.gz\\n\")\n",
    "    f.write(\"SIX5\\tSIX5.narrowPeak.gz\\t\\tSIX5.ambiguous.gz\\n\")\n",
    "    f.write(\"ZNF143\\tZNF143.narrowPeak.gz\\t\\tZNF143.ambiguous.gz\\n\")\n",
    "f.close() \n",
    "! cat tasks.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* Each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. \n",
    "\n",
    "* The bin is labeled ambiguous (label = -1) and excluded from training if there is some overlap with the narrowPeak, but the peak summit does not lie in that overlap. \n",
    "\n",
    "* The bin is labeled negative if there is no overlap with the narrowPeak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 7)\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 7)\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 7)\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 7)\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 7)\n",
      "got peak subset for chrom:chr22 for task:SPI1\n",
      "got peak subset for chrom:chr21 for task:SPI1\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 7)\n",
      "finished chromosome:chr22 for task:SPI1got peak subset for chrom:chr20 for task:SPI1\n",
      "\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 7)\n",
      "finished chromosome:chr20 for task:SPI1finished chromosome:chr21 for task:SPI1\n",
      "pre-allocated df for chrom:chr13with dimensions:(2303378, 7)\n",
      "\n",
      "got peak subset for chrom:chr17 for task:SPI1\n",
      "pre-allocated df for chrom:chr14with dimensions:(2146971, 7)\n",
      "pre-allocated df for chrom:chr11with dimensions:(2700111, 7)\n",
      "got peak subset for chrom:chr16 for task:SPI1\n",
      "finished chromosome:chr17 for task:SPI1\n",
      "got peak subset for chrom:chr18 for task:SPI1\n",
      "pre-allocated df for chrom:chr12with dimensions:(2677018, 7)\n",
      "got peak subset for chrom:chr15 for task:SPI1\n",
      "finished chromosome:chr18 for task:SPI1\n",
      "pre-allocated df for chrom:chr10with dimensions:(2710675, 7)\n",
      "pre-allocated df for chrom:chr9with dimensions:(2824249, 7)\n",
      "got peak subset for chrom:chr13 for task:SPI1\n",
      "finished chromosome:chr16 for task:SPI1\n",
      "got peak subset for chrom:chr14 for task:SPI1\n",
      "finished chromosome:chr15 for task:SPI1\n",
      "finished chromosome:chr13 for task:SPI1\n",
      "pre-allocated df for chrom:chrXwith dimensions:(3105392, 7)\n",
      "got peak subset for chrom:chr11 for task:SPI1\n",
      "finished chromosome:chr14 for task:SPI1\n",
      "pre-allocated df for chrom:chr8with dimensions:(2927261, 7)\n",
      "got peak subset for chrom:chr12 for task:SPI1\n",
      "pre-allocated df for chrom:chr6with dimensions:(3422282, 7)\n",
      "got peak subset for chrom:chr9 for task:SPI1\n",
      "got peak subset for chrom:chr10 for task:SPI1\n",
      "finished chromosome:chr11 for task:SPI1\n",
      "finished chromosome:chr12 for task:SPI1\n",
      "pre-allocated df for chrom:chr5with dimensions:(3618286, 7)\n",
      "pre-allocated df for chrom:chr7with dimensions:(3182754, 7)\n",
      "pre-allocated df for chrom:chr4with dimensions:(3823066, 7)\n",
      "finished chromosome:chr9 for task:SPI1\n",
      "got peak subset for chrom:chrX for task:SPI1\n",
      "got peak subset for chrom:chr8 for task:SPI1\n",
      "finished chromosome:chr10 for task:SPI1\n",
      "finished chromosome:chrX for task:SPI1got peak subset for chrom:chr6 for task:SPI1\n",
      "got peak subset for chrom:chr22 for task:CTCF\n",
      "got peak subset for chrom:chr21 for task:CTCF\n",
      "\n",
      "finished chromosome:chr21 for task:CTCF\n",
      "got peak subset for chrom:chr7 for task:SPI1\n",
      "finished chromosome:chr8 for task:SPI1\n",
      "finished chromosome:chr22 for task:CTCF\n",
      "got peak subset for chrom:chr20 for task:CTCF\n",
      "pre-allocated df for chrom:chr3with dimensions:(3960429, 7)\n",
      "got peak subset for chrom:chr5 for task:SPI1\n",
      "finished chromosome:chr6 for task:SPI1\n",
      "got peak subset for chrom:chr16 for task:CTCF\n",
      "got peak subset for chrom:chr4 for task:SPI1\n",
      "finished chromosome:chr5 for task:SPI1\n",
      "finished chromosome:chr20 for task:CTCF\n",
      "got peak subset for chrom:chr17 for task:CTCF\n",
      "finished chromosome:chr4 for task:SPI1\n",
      "got peak subset for chrom:chr18 for task:CTCF\n",
      "finished chromosome:chr7 for task:SPI1\n",
      "got peak subset for chrom:chr3 for task:SPI1\n",
      "finished chromosome:chr16 for task:CTCFgot peak subset for chrom:chr21 for task:SIX5\n",
      "\n",
      "finished chromosome:chr18 for task:CTCFgot peak subset for chrom:chr11 for task:CTCF\n",
      "got peak subset for chrom:chr15 for task:CTCF\n",
      "\n",
      "got peak subset for chrom:chr22 for task:SIX5\n",
      "got peak subset for chrom:chr20 for task:SIX5\n",
      "got peak subset for chrom:chr13 for task:CTCF\n",
      "got peak subset for chrom:chr14 for task:CTCF\n",
      "finished chromosome:chr3 for task:SPI1\n",
      "finished chromosome:chr21 for task:SIX5\n",
      "finished chromosome:chr22 for task:SIX5\n",
      "got peak subset for chrom:chr17 for task:SIX5\n",
      "finished chromosome:chr20 for task:SIX5\n",
      "finished chromosome:chr17 for task:CTCFfinished chromosome:chr15 for task:CTCF\n",
      "\n",
      "got peak subset for chrom:chr12 for task:CTCF\n",
      "finished chromosome:chr13 for task:CTCF\n",
      "finished chromosome:chr14 for task:CTCF\n",
      "finished chromosome:chr11 for task:CTCF\n",
      "got peak subset for chrom:chr9 for task:CTCF\n",
      "got peak subset for chrom:chr10 for task:CTCF\n",
      "finished chromosome:chr17 for task:SIX5\n",
      "finished chromosome:chr12 for task:CTCF\n",
      "finished chromosome:chr9 for task:CTCF\n",
      "got peak subset for chrom:chr18 for task:SIX5\n",
      "got peak subset for chrom:chrX for task:CTCF\n",
      "finished chromosome:chr10 for task:CTCF\n",
      "got peak subset for chrom:chr13 for task:SIX5\n",
      "got peak subset for chrom:chr16 for task:SIX5\n",
      "finished chromosome:chr18 for task:SIX5\n",
      "got peak subset for chrom:chr11 for task:SIX5\n",
      "finished chromosome:chrX for task:CTCF\n",
      "finished chromosome:chr16 for task:SIX5\n",
      "got peak subset for chrom:chr8 for task:CTCF\n",
      "finished chromosome:chr13 for task:SIX5\n",
      "got peak subset for chrom:chr6 for task:CTCF\n",
      "got peak subset for chrom:chr5 for task:CTCF\n",
      "got peak subset for chrom:chr14 for task:SIX5\n",
      "finished chromosome:chr11 for task:SIX5\n",
      "got peak subset for chrom:chr15 for task:SIX5\n",
      "got peak subset for chrom:chr10 for task:SIX5\n",
      "got peak subset for chrom:chr12 for task:SIX5\n",
      "got peak subset for chrom:chrX for task:SIX5\n",
      "finished chromosome:chr14 for task:SIX5finished chromosome:chr8 for task:CTCF\n",
      "got peak subset for chrom:chr9 for task:SIX5\n",
      "finished chromosome:chr6 for task:CTCF\n",
      "\n",
      "finished chromosome:chr5 for task:CTCF\n",
      "got peak subset for chrom:chr7 for task:CTCF\n",
      "finished chromosome:chr15 for task:SIX5\n",
      "got peak subset for chrom:chr8 for task:SIX5\n",
      "got peak subset for chrom:chr4 for task:CTCF\n",
      "finished chromosome:chr12 for task:SIX5\n",
      "got peak subset for chrom:chr3 for task:CTCF\n",
      "finished chromosome:chrX for task:SIX5\n",
      "finished chromosome:chr10 for task:SIX5\n",
      "finished chromosome:chr9 for task:SIX5\n",
      "finished chromosome:chr8 for task:SIX5\n",
      "got peak subset for chrom:chr6 for task:SIX5\n",
      "finished chromosome:chr7 for task:CTCF\n",
      "finished chromosome:chr4 for task:CTCF\n",
      "finished chromosome:chr3 for task:CTCF\n",
      "got peak subset for chrom:chr7 for task:SIX5\n",
      "got peak subset for chrom:chr5 for task:SIX5\n",
      "finished chromosome:chr6 for task:SIX5\n",
      "got peak subset for chrom:chr4 for task:SIX5\n",
      "finished chromosome:chr7 for task:SIX5\n",
      "finished chromosome:chr5 for task:SIX5\n",
      "finished chromosome:chr4 for task:SIX5\n",
      "got peak subset for chrom:chr3 for task:SIX5\n",
      "finished chromosome:chr3 for task:SIX5\n",
      "got peak subset for chrom:chr21 for task:ZNF143\n",
      "got peak subset for chrom:chr20 for task:ZNF143\n",
      "finished chromosome:chr21 for task:ZNF143\n",
      "finished chromosome:chr20 for task:ZNF143\n",
      "got peak subset for chrom:chr22 for task:ZNF143\n",
      "got peak subset for chrom:chr17 for task:ZNF143\n",
      "finished chromosome:chr22 for task:ZNF143\n",
      "got peak subset for chrom:chr18 for task:ZNF143\n",
      "got peak subset for chrom:chr11 for task:ZNF143\n",
      "got peak subset for chrom:chr14 for task:ZNF143\n",
      "got peak subset for chrom:chr13 for task:ZNF143\n",
      "finished chromosome:chr18 for task:ZNF143\n",
      "finished chromosome:chr17 for task:ZNF143\n",
      "got peak subset for chrom:chr12 for task:ZNF143\n",
      "got peak subset for chrom:chr16 for task:ZNF143\n",
      "got peak subset for chrom:chr15 for task:ZNF143\n",
      "finished chromosome:chr13 for task:ZNF143\n",
      "got peak subset for chrom:chr9 for task:ZNF143\n",
      "got peak subset for chrom:chr10 for task:ZNF143\n",
      "pre-allocated df for chrom:chrYwith dimensions:(1187452, 7)\n",
      "finished chromosome:chr11 for task:ZNF143\n",
      "finished chromosome:chr14 for task:ZNF143\n",
      "finished chromosome:chr15 for task:ZNF143\n",
      "finished chromosome:chr16 for task:ZNF143\n",
      "finished chromosome:chr12 for task:ZNF143\n",
      "got peak subset for chrom:chrX for task:ZNF143\n",
      "got peak subset for chrom:chr8 for task:ZNF143\n",
      "finished chromosome:chr9 for task:ZNF143\n",
      "finished chromosome:chr10 for task:ZNF143\n",
      "got peak subset for chrom:chr7 for task:ZNF143\n",
      "got peak subset for chrom:chrY for task:SPI1\n",
      "got peak subset for chrom:chr5 for task:ZNF143\n",
      "got peak subset for chrom:chr4 for task:ZNF143\n",
      "finished chromosome:chrY for task:SPI1\n",
      "finished chromosome:chrX for task:ZNF143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished chromosome:chr8 for task:ZNF143\n",
      "got peak subset for chrom:chr3 for task:ZNF143\n",
      "finished chromosome:chr7 for task:ZNF143\n",
      "got peak subset for chrom:chr6 for task:ZNF143\n",
      "finished chromosome:chr4 for task:ZNF143\n",
      "finished chromosome:chr5 for task:ZNF143\n",
      "finished chromosome:chr3 for task:ZNF143\n",
      "finished chromosome:chr6 for task:ZNF143\n",
      "got peak subset for chrom:chrY for task:CTCF\n",
      "finished chromosome:chrY for task:CTCF\n",
      "got peak subset for chrom:chrY for task:SIX5\n",
      "finished chromosome:chrY for task:SIX5\n",
      "got peak subset for chrom:chrY for task:ZNF143\n",
      "finished chromosome:chrY for task:ZNF143\n",
      "writing output chromosomes:chr3\n",
      "writing output chromosomes:chr4\n",
      "writing output chromosomes:chr5\n",
      "writing output chromosomes:chr6\n",
      "writing output chromosomes:chr7\n",
      "writing output chromosomes:chr8\n",
      "writing output chromosomes:chr9\n",
      "writing output chromosomes:chr10\n",
      "writing output chromosomes:chr11\n",
      "writing output chromosomes:chr12\n",
      "writing output chromosomes:chr13\n",
      "writing output chromosomes:chr14\n",
      "writing output chromosomes:chr15\n",
      "writing output chromosomes:chr16\n",
      "writing output chromosomes:chr17\n",
      "writing output chromosomes:chr18\n",
      "writing output chromosomes:chr20\n",
      "writing output chromosomes:chr21\n",
      "writing output chromosomes:chr22\n",
      "writing output chromosomes:chrX\n",
      "writing output chromosomes:chrY\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr22with dimensions:(1026072, 7)\n",
      "pre-allocated df for chrom:chr21with dimensions:(962578, 7)\n",
      "pre-allocated df for chrom:chr16with dimensions:(1807076, 7)\n",
      "pre-allocated df for chrom:chr20with dimensions:(1260491, 7)\n",
      "pre-allocated df for chrom:chr18with dimensions:(1561525, 7)\n",
      "pre-allocated df for chrom:chr17with dimensions:(1623885, 7)\n",
      "pre-allocated df for chrom:chr15with dimensions:(2050608, 7)\n",
      "got peak subset for chrom:chr22 for task:SPI1\n",
      "got peak subset for chrom:chr21 for task:SPI1\n"
     ]
    }
   ],
   "source": [
    "#we will include all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "#1) Generate genome-wide negatives in addition to positives \n",
    "train_set_params={\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.train.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'    \n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "\n",
    "positives_train_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.train.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_train_set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr1with dimensions:(4984993, 7)\n",
      "got peak subset for chrom:chr1 for task:SPI1\n",
      "finished chromosome:chr1 for task:SPI1\n",
      "got peak subset for chrom:chr1 for task:CTCF\n",
      "finished chromosome:chr1 for task:CTCF\n",
      "got peak subset for chrom:chr1 for task:SIX5\n",
      "finished chromosome:chr1 for task:SIX5\n",
      "got peak subset for chrom:chr1 for task:ZNF143\n",
      "finished chromosome:chr1 for task:ZNF143\n",
      "writing output chromosomes:chr1\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr1with dimensions:(4984993, 7)\n",
      "got peak subset for chrom:chr1 for task:SPI1\n",
      "finished chromosome:chr1 for task:SPI1\n",
      "got peak subset for chrom:chr1 for task:CTCF\n",
      "finished chromosome:chr1 for task:CTCF\n",
      "got peak subset for chrom:chr1 for task:SIX5\n",
      "finished chromosome:chr1 for task:SIX5\n",
      "got peak subset for chrom:chr1 for task:ZNF143\n",
      "finished chromosome:chr1 for task:ZNF143\n",
      "writing output chromosomes:chr1\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#We will include chromsome 1 in our validation set \n",
    "\n",
    "#1) Generate genome-wide negatives in addition to positives \n",
    "valid_set_params={'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.valid.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "positives_valid_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.valid.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_valid_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dictionary of bed files and bigwig files for each task:\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr19with dimensions:(1182560, 7)\n",
      "got peak subset for chrom:chr19 for task:SPI1\n",
      "finished chromosome:chr19 for task:SPI1\n",
      "got peak subset for chrom:chr19 for task:CTCF\n",
      "finished chromosome:chr19 for task:CTCF\n",
      "got peak subset for chrom:chr19 for task:SIX5\n",
      "finished chromosome:chr19 for task:SIX5\n",
      "pre-allocated df for chrom:chr2with dimensions:(4863968, 7)\n",
      "got peak subset for chrom:chr2 for task:SPI1\n",
      "finished chromosome:chr2 for task:SPI1\n",
      "got peak subset for chrom:chr19 for task:ZNF143\n",
      "finished chromosome:chr19 for task:ZNF143\n",
      "got peak subset for chrom:chr2 for task:CTCF\n",
      "finished chromosome:chr2 for task:CTCF\n",
      "got peak subset for chrom:chr2 for task:SIX5\n",
      "finished chromosome:chr2 for task:SIX5\n",
      "got peak subset for chrom:chr2 for task:ZNF143\n",
      "finished chromosome:chr2 for task:ZNF143\n",
      "writing output chromosomes:chr2\n",
      "writing output chromosomes:chr19\n",
      "done!\n",
      "creating dictionary of bed files and bigwig files for each task:\n",
      "CTCF\n",
      "No BigWig file was provided for task:CTCF; Make sure this is intentional\n",
      "SPI1\n",
      "No BigWig file was provided for task:SPI1; Make sure this is intentional\n",
      "SIX5\n",
      "No BigWig file was provided for task:SIX5; Make sure this is intentional\n",
      "ZNF143\n",
      "No BigWig file was provided for task:ZNF143; Make sure this is intentional\n",
      "creating chromosome thread pool\n",
      "launching thread pool\n",
      "pre-allocated df for chrom:chr19with dimensions:(1182560, 7)\n",
      "got peak subset for chrom:chr19 for task:SPI1\n",
      "finished chromosome:chr19 for task:SPI1\n",
      "got peak subset for chrom:chr19 for task:CTCF\n",
      "finished chromosome:chr19 for task:CTCF\n",
      "got peak subset for chrom:chr19 for task:SIX5\n",
      "finished chromosome:chr19 for task:SIX5\n",
      "pre-allocated df for chrom:chr2with dimensions:(4863968, 7)\n",
      "got peak subset for chrom:chr2 for task:SPI1\n",
      "finished chromosome:chr2 for task:SPI1\n",
      "got peak subset for chrom:chr2 for task:CTCF\n",
      "got peak subset for chrom:chr2 for task:SIX5\n",
      "finished chromosome:chr2 for task:CTCF\n",
      "got peak subset for chrom:chr19 for task:ZNF143\n",
      "finished chromosome:chr2 for task:SIX5\n",
      "finished chromosome:chr19 for task:ZNF143\n",
      "got peak subset for chrom:chr2 for task:ZNF143\n",
      "finished chromosome:chr2 for task:ZNF143\n",
      "writing output chromosomes:chr2\n",
      "writing output chromosomes:chr19\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#We will include chromosomes 2 and 19 in our testing set \n",
    "test_set_params={\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.test.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "positives_test_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.test.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"TF.train.hdf5\",start=0,stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When provided with the --store-positives_only flag, the code generates all bins for each task that are labeled positive.\n",
    "pd.read_hdf(\"SPI1.positives.TF.train.hdf5\",start=0,stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load our test set labels into memory here, as we will use them to measure performance in cases 1 - 4 below.\n",
    "#Note that we only load the labels into memory, not the actual test dataset. \n",
    "\n",
    "#It is not necessary to load the training/validation dataset or labels, see below. \n",
    "test_set=pd.read_hdf(\"TF.test.hdf5\")\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of in vivo data : batch generators and class upsampling <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "In tutorials 1 - 3, we used the [keras fit](https://keras.io/models/sequential/#fit) function to train a CNN. However, when working with real data we face two new challenges: \n",
    "\n",
    "1) The dataset is much bigger. In our training set, there are 50,881,560 1kb bins, in our validation set, there are 4,984,994 bins, and in our test set there are 6,046,529 bins. Loading this dataset into memory to pass as a numpy array to the CNN code will require more memory than is available on many machines. Consequently, we use the [keras fit_generator](https://keras.io/models/sequential/#fit_generator) function to limit the memory footprint. This function reads in one batch of training and one batch of validation data at a time from a python generator. in *dragonn.generators*, we provide several python generator functions to match the scenarios below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.generators import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The dataset is highly imbalanced. Of the 50,881,559 1kb bins in the training set, only \n",
    "\n",
    "* 138,399 are labeled positive for the SPI1 task (367 negatives: 1 positive )\n",
    "\n",
    "* 131,033 are labeled positive for the CTCF task (388 negatives: 1 positive ) \n",
    "\n",
    "* 88,541 are labeled positive for the ZNF143 task (574 negatives: 1 positive ) \n",
    "\n",
    "* 15,630 are labeled positive for the SIX5 task (3255 negatives: 1 positive ) \n",
    "\n",
    "The class imbalance is far too high for the model to learn unassisted. Hence, we upsample the positive bins to include in each batch with the \"upsample\" argument to DataGenerator. The upsample argument accepts a fraction between 0 and 1 and ensures that this fraction of the batch consists of positive bins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History, TensorBoard \n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "#we use a custom binary cross-entropy loss that can handle ambiguous labels (denoted with -1 ) and exclude them \n",
    "# from the loss calculation \n",
    "from dragonn.custom_losses import get_ambig_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "def initialize_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    #use the custom ambig_binary_crossentropy loss, indicating that a value of -1 indicates an ambiguous label \n",
    "    loss=get_ambig_binary_crossentropy(-1)\n",
    "    \n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam', loss=loss,\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pre-generated models and test set predictions (optional) <a name='3.5'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "#### **Note: on a GPU, each of the models below should take 10 - 15 minutes to train, with an additional 7-10 minutesu for prediction on the corresponding test sets. Although training the models is a valuable exercise, for the sake of time, we also provide hdf5 files with pre-trained models and test set predictions. These were generated by running the tutorial and saving the outputs at the end (see <a href=#10>Save tutorial outputs</a>). Uncomment the cell below if you wish to load the pre-trained models and test set predictions to use for performance comparison and DeepLIFT analyis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the models \n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case1_spi1_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case1_ctcf_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case2_spi1_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case2_ctcf_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case3_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case4_spi1_model.hdf5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the models from hdf5 \n",
    "#from keras.models import load_model \n",
    "#case1_spi1_model=load_model(\"case1_spi1_model.hdf5\")\n",
    "#case1_ctcf_model=load_model(\"case1_ctcf_model.hdf5\")\n",
    "#case2_spi1_model=load_model(\"case2_spi1_model.hdf5\")\n",
    "#case2_ctcf_model=load_model(\"case2_ctcf_model.hdf5\")\n",
    "#case3_model=load_model(\"case3_model.hdf5\")\n",
    "#case4_spi1_model=load_model(\"case4_spi1_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Download test set predictions \n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/test_set_predictions.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract predictions from hdf5 for each model \n",
    "#import h5py\n",
    "#test_set_predictions=h5py.File(\"test_set_predictions.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case1_spi1_test_predictions=test_set_predictions[\"case1_spi1_test_predictions\"].value\n",
    "#case1_ctcf_test_predictions=test_set_predictions[\"case1_spi1_test_predictions\"].value\n",
    "#case2_spi1_test_predictions=test_set_predictions[\"case2_spi1_test_predictions\"].value\n",
    "#case2_ctcf_test_predictions=test_set_predictions[\"case2_ctcf_test_predictions\"].value\n",
    "#case3_test_predictions=test_set_predictions[\"case3_test_predictions\"].value\n",
    "#case4_spi1_test_predictions=test_set_predictions[\"case4_spi1_test_predictions\"].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case1_spi1_test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Negatives consist of shuffled references, single-tasked models<a name='4'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by training a model on the SPI1 and CTCF dataset with the following specifications: \n",
    "\n",
    "* We use dinucleotide-shuffled positive bins as the negative set. \n",
    "\n",
    "* Each batch contains one-hot encoded 1kb regions from the genome, as well as the one-hot-encoded reverse complement sequences of those regions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create generators for the training and validation data to meet these specifications: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "from dragonn.generators import * \n",
    "case1_spi1_train_gen=DataGenerator(\"SPI1.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
    "case1_spi1_valid_gen=DataGenerator(\"SPI1.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
    "case1_ctcf_train_gen=DataGenerator(\"CTCF.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
    "case1_ctcf_valid_gen=DataGenerator(\"CTCF.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow the standard protocol we used in tutorials 1 - 3 to train a keras model, with the exception that we use the fit_generator function in keras, rather than the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case1_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case1_spi1=case1_spi1_model.fit_generator(case1_spi1_train_gen,\n",
    "                                                  validation_data=case1_spi1_valid_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the CTCF model \n",
    "case1_ctcf_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case1_ctcf=case1_ctcf_model.fit_generator(case1_ctcf_train_gen,\n",
    "                                                  validation_data=case1_ctcf_valid_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_case1_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curve for CTCF \n",
    "plot_learning_curve(history_case1_ctcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now measure how well the models performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case1_spi1_test_predictions=case1_spi1_model.predict_generator(case1_spi1_test_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_ctcf_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['CTCF'])\n",
    "case1_ctcf_test_predictions=case1_ctcf_model.predict_generator(case1_ctcf_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format true & predicted test labels for performance assessment \n",
    "\n",
    "#if test_set.shape is not a multiple of batch_size, \n",
    "#there may be some extra values in test_set that need to get truncated.\n",
    "spi1_test_truth=np.expand_dims(test_set['SPI1'][0:case1_spi1_test_predictions.shape[0]],1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_truth,case1_spi1_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcf_test_truth=np.expand_dims(test_set['CTCF'][0:case1_ctcf_test_predictions.shape[0]],1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(ctcf_test_truth,case1_ctcf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Whole-genome negatives, single-tasked models <a name='5'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "* Since the genomewide imbalance of negative bins to positive bins is very high, we upsample each batch in the training set to contain 30% positive samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "from dragonn.generators import * \n",
    "case2_spi1_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample_ratio=0.3,batch_size=256)\n",
    "case2_spi1_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False,batch_size=256)\n",
    "case2_ctcf_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"CTCF\"],upsample_ratio=0.3,batch_size=256)\n",
    "case2_ctcf_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"CTCF\"],upsample=False,batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case2_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case2_spi1=case2_spi1_model.fit_generator(case2_spi1_train_gen,\n",
    "                                                  validation_data=case2_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the CTCF model \n",
    "case2_ctcf_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case2_ctcf=case2_ctcf_model.fit_generator(case2_ctcf_train_gen,\n",
    "                                                  validation_data=case2_ctcf_valid_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "plot_learning_curve(history_case2_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for CTCF  \n",
    "plot_learning_curve(history_case2_ctcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get model predictions on the test set \n",
    "case2_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case2_spi1_test_predictions=case2_spi1_model.predict_generator(case2_spi1_test_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_ctcf_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['CTCF'])\n",
    "case2_ctcf_test_predictions=case2_ctcf_model.predict_generator(case2_ctcf_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.metrics import ClassificationResult\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(spi1_test_truth,case2_spi1_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(ctcf_test_truth,case2_ctcf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Whole-genome negatives, multi-tasked models <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators for multi-tasked models. Guarantee 10% positives in each batch \n",
    "case3_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,batch_size=256)\n",
    "case3_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",upsample=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case3_model=initialize_model(4)\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case3=case3_model.fit_generator(case3_train_gen,\n",
    "                                        validation_data=case3_valid_gen,\n",
    "                                        steps_per_epoch=10000,\n",
    "                                        validation_steps=5000,\n",
    "                                        epochs=150,\n",
    "                                        verbose=1,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=40,\n",
    "                                        max_queue_size=100,\n",
    "                                        callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for the multi-tasked model   \n",
    "plot_learning_curve(history_case3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case3_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                             \"hg19.genome.fa.gz\",\n",
    "                             upsample=False,\n",
    "                             add_revcomp=False,\n",
    "                             batch_size=1000)\n",
    "case3_test_predictions=case3_model.predict_generator(case3_test_gen,\n",
    "                                                     max_queue_size=5000, \n",
    "                                                     workers=40, \n",
    "                                                     use_multiprocessing=True, \n",
    "                                                     verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth=test_set[0:case3_test_predictions.shape[0]].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['SPI1'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,0],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['CTCF'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,1],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['ZNF143'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,2],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['SIX5'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,3],1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: What happens if we don't upsample positive examples in our batches? <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "case4_spi1_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False,batch_size=256)\n",
    "case4_spi1_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case4_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case4_spi1=case4_spi1_model.fit_generator(case4_spi1_train_gen,\n",
    "                                                  validation_data=case4_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "plot_learning_curve(history_case4_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We  use a custom batch_predict function to generate predictions on the test set, one batch at a time \n",
    "case4_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case4_spi1_test_predictions=case4_spi1_model.predict_generator(case4_spi1_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(spi1_test_truth,case4_spi1_test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='8'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "The highest test set auPRC was observed in Case 2 -- the single-tasked, genome-wide, SPI1 model with upsampling of positives. Let's run DeepLIFT on some high-confidence true positives. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
    "true_pos_spi1=test_truth[spi1_test_truth*case2_spi1_test_predictions >0.9]\n",
    "true_pos_spi1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(case2_spi1_model,deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the DeepLIFT tracks and see if the model successfully learned SPI1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import  plot_seq_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in to the center of one sequence so that it is easier to distinguish the motif: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[450:550],deep_lift_input_spi1[2].squeeze()[450:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we query the sequence \"TCACTTCCCCTT\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match for SPIB (p-value = 8.93e-7): \n",
    "<img src=\"tutorial_images/SPI1.Tut4.png\" alt=\"SPI1TomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the high-confidence true positives from the CTCF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence predictions)\n",
    "true_pos_ctcf=test_truth[ctcf_test_truth*case2_ctcf_test_predictions >0.9]\n",
    "true_pos_ctcf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_input_ctcf=one_hot_from_bed([i for i in true_pos_ctcf.index],\"hg19.genome.fa.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_ctcf=deeplift(case2_ctcf_model,deep_lift_input_ctcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[0],deep_lift_input_ctcf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[1],deep_lift_input_ctcf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[2],deep_lift_input_ctcf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[0].squeeze()[500:600],deep_lift_input_ctcf[0].squeeze()[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the sequence \"GCGCCCTCTGCTGG\" in TomTom confirms a strong match (p=1.43e-6) to the canonical CTCF motif:\n",
    "<img src=\"tutorial_images/CTCF.Tut4.png\" alt=\"CTCFTomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis of *in vivo* transcription factor ChiP-seq datasets, we draw the following conclusions: \n",
    "\n",
    "* Training the models genome-wide by splitting the genome into short (200 bp), overlapping (stride=50) bins leads to improved generalization performance on the test set as compared to \"easier\" negative sets. I.E we observed superior test set performance for both the SPI1 and CTCF tasks when training on the whole genome as compared to using shuffled reference negatives. \n",
    "\n",
    "\n",
    "* Due to high class imbalance in *in vivo* data, it is necessary to upsample positive examples in each batch to facilitate model training. We found that upsampling positives to constitute 30% of each batch worked well --i.e. interpretation with DeepLIFT indicates the model learned the correct motif in CTCF and SPI1 ChIP-seq datasets. What happens if you alter the *upsample_ratio* parameter in the generator function? Does the model learn equally well with *upsample_ratio=0.1*? Does it learn better or worse with *upsample_ratio=0.5*? \n",
    "\n",
    "\n",
    "* Multi-tasking can lead to improved performance for tasks that are similar (i.e. CTCF/ZNF143/SIX5 are similar motifs). However, multi-tasking does not improve performance on tasks that are different (i.e. the SPI1 motif does not resemble the other three motifs, so multi-tasking does not improve performance for the SPI1 task). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tutorial outputs <a name='10'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "We save the models and test set predictions generated in this tutorial to an hdf5 file so that they can be loaded more readily in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models \n",
    "case1_spi1_model.save(\"case1_spi1_model.hdf5\")\n",
    "case1_ctcf_model.save(\"case1_ctcf_model.hdf5\")\n",
    "case2_spi1_model.save(\"case2_spi1_model.hdf5\")\n",
    "case2_ctcf_model.save(\"case2_ctcf_model.hdf5\")\n",
    "case3_model.save(\"case3_model.hdf5\")\n",
    "case4_spi1_model.save(\"case4_spi1_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the test set predictions \n",
    "import h5py \n",
    "test_set_predictions=h5py.File(\"test_set_predictions.hdf5\",'w')\n",
    "test_set_predictions.create_dataset(\"case1_spi1_test_predictions\",data=case1_spi1_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case1_ctcf_test_predictions\",data=case1_ctcf_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case2_spi1_test_predictions\",data=case2_spi1_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case2_ctcf_test_predictions\",data=case2_ctcf_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case3_test_predictions\",data=case3_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case4_spi1_test_predictions\",data=case4_spi1_test_predictions)\n",
    "test_set_predictions.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
